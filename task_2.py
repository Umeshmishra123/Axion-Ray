# -*- coding: utf-8 -*-
"""Task-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PhDBiU6INyKms1tAfKlPBy0fDVx5IhQX

**Task-2**
"""

import pandas as pd

# Load the Excel file
file_path = '/content/Task 2.xlsx'  # Replace this with your file path
data = pd.read_excel(file_path)

# Display the first few rows of the dataset
print(data.head())

# Show basic information about the dataset
print(data.info())

# Column-wise summary of the dataset
column_summary = pd.DataFrame({
    "Column Name": data.columns,
    "Data Type": data.dtypes,
    "Non-Null Count": data.count(),  # Counts non-missing values
    "Unique Values": data.nunique(),  # Counts unique values
})

# Display the column-wise summary
print(column_summary.sort_values(by="Non-Null Count", ascending=False).reset_index(drop=True))

"""**Data Cleaning**

Data cleaning ensures that our dataset is consistent, free from missing or incorrect values, and ready for analysis.now we'll clean it:

3.1 Handle Missing Values
Drop columns with more than 50% missing values since they may not be useful.
For numerical columns, fill missing values with the median (less sensitive to outliers).
For categorical columns, fill missing values with the mode (most frequent value).

3.2 Standardize Categorical Data
Use FuzzyWuzzy to handle inconsistent labels (e.g., typos or capitalization).

3.3 Remove Outliers
Use the IQR method (Interquartile Range) to cap extreme values in numerical columns like KM and TOTALCOST.
"""

!pip install fuzzywuzzy

from fuzzywuzzy import process, fuzz
import numpy as np

# Copy the data for cleaning
data_cleaned = data.copy()

# 3.1 Handle Missing Values
# Drop columns with more than 50% missing values
data_cleaned = data_cleaned.dropna(thresh=len(data_cleaned) * 0.5, axis=1)

# Fill numerical columns with median
numerical_cols = data_cleaned.select_dtypes(include=["int64", "float64"]).columns
for col in numerical_cols:
    data_cleaned[col] = data_cleaned[col].fillna(data_cleaned[col].median())

# Fill categorical columns with mode
categorical_cols = data_cleaned.select_dtypes(include=["object"]).columns
for col in categorical_cols:
    data_cleaned[col] = data_cleaned[col].fillna(data_cleaned[col].mode()[0])

# Convert categorical columns to strings to avoid issues with FuzzyWuzzy
for col in categorical_cols:
    data_cleaned[col] = data_cleaned[col].astype(str)

# Cleanse non-string or unexpected characters in categorical columns
for col in categorical_cols:
    data_cleaned[col] = data_cleaned[col].apply(lambda x: ''.join(filter(str.isprintable, x)))

# Standardize Categorical Data with FuzzyWuzzy
for col in categorical_cols:
    unique_values = data_cleaned[col].dropna().unique()
    standardized_values = {val: process.extractOne(str(val), unique_values, scorer=fuzz.token_sort_ratio)[0] for val in unique_values}
    data_cleaned[col] = data_cleaned[col].map(standardized_values)

# Fill remaining NaN/missing values with a placeholder
for col in categorical_cols:
    data_cleaned[col] = data_cleaned[col].fillna("Unknown")

"""Identifying Critical Columns and Visualizing Insights

Identifying Critical Columns
Objective:
Select the top 5 critical columns that provide the most valuable insights for stakeholders. These columns should be significant in understanding the dataset's core aspects, such as financial impact, operational efficiency, customer satisfaction, etc.

Approach:

Understand Stakeholder Needs:
Determine what information is most valuable. For example:

Financial Impact: TOTALCOST
Operational Efficiency: REPAIR_DATE, KM
Customer Satisfaction: CUSTOMER_VERBATIM
Common Issues: CAUSAL_PART_NM
Product Segmentation: PLATFORM
Analyze Data Types and Distributions:
Ensure selected columns have meaningful distributions and are not skewed excessively unless thatâ€™s part of the insight.

Correlation Analysis (Optional):
Check for correlations between numerical columns to avoid redundancy.

Selected Top 5 Critical Columns:

TOTALCOST
Represents the financial impact of each transaction.

PLATFORM
Categorizes different vehicle segments or platforms.

REPAIR_DATE
Tracks when repairs occur, indicating operational timelines.

CAUSAL_PART_NM
Identifies recurring parts/issues leading to repairs.

CUSTOMER_VERBATIM
Captures customer feedback and sentiment.

**Visualizing Insights**
"""



"""Objective:
Create visualizations to effectively communicate the insights derived from the critical columns.

Tools Used:

Matplotlib: Fundamental plotting library.
Seaborn: High-level interface for attractive statistical graphics.

**Average Total Cost by Platform**

*Understanding which platforms incur higher repair costs can help in budgeting and resource allocation.*
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set the style for seaborn
sns.set(style="whitegrid")

# Calculate average total cost per platform
avg_total_cost = data_cleaned.groupby('PLATFORM')['TOTALCOST'].mean().reset_index()

# Sort platforms by average total cost
avg_total_cost = avg_total_cost.sort_values(by='TOTALCOST', ascending=False)

# Create a bar plot
plt.figure(figsize=(12, 6))
sns.barplot(data=avg_total_cost, x='PLATFORM', y='TOTALCOST', palette='viridis')
plt.title('Average Total Cost by Platform')
plt.xlabel('Platform')
plt.ylabel('Average Total Cost')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Explanation:

Grouping: Aggregates TOTALCOST by PLATFORM.

Sorting: Orders platforms from highest to lowest average cost for clarity.

Visualization: A bar plot highlights which platforms are costlier on average.

**Distribution of Repair Age**

*Analyzing the distribution of repair ages helps in understanding the lifecycle and durability of products.*
"""

# Check if 'REPAIR_AGE' exists and is numerical
if 'REPAIR_AGE' in data_cleaned.columns:
    plt.figure(figsize=(10, 6))
    sns.histplot(data_cleaned['REPAIR_AGE'], bins=20, kde=True, color='skyblue')
    plt.title('Distribution of Repair Age')
    plt.xlabel('Repair Age (Years)')
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()
else:
    print("'REPAIR_AGE' column is not present in the dataset.")

"""Explanation:

Histogram with KDE: Shows the frequency distribution and density of repair ages.

Insights: Identifies common repair ages and any skewness in the data.

**Top 10 Most Frequent Causal Parts**

*Identifying the most common parts leading to repairs can guide quality improvement initiatives.*
"""

# Check if 'CAUSAL_PART_NM' exists
if 'CAUSAL_PART_NM' in data_cleaned.columns:
    top_parts = data_cleaned['CAUSAL_PART_NM'].value_counts().head(10).reset_index()
    top_parts.columns = ['Causal Part Name', 'Frequency']

    plt.figure(figsize=(12, 6))
    sns.barplot(data=top_parts, x='Frequency', y='Causal Part Name', palette='magma')
    plt.title('Top 10 Most Frequent Causal Parts')
    plt.xlabel('Frequency')
    plt.ylabel('Causal Part Name')
    plt.tight_layout()
    plt.show()
else:
    print("'CAUSAL_PART_NM' column is not present in the dataset.")

"""Explanation:

Value Counts: Identifies the top 10 most common parts causing issues.

Bar Plot: Visual representation makes it easy to spot the most problematic parts.

**Convert REPAIR_DATE to Datetime Format**
"""

# Ensure 'REPAIR_DATE' is in datetime format
if 'REPAIR_DATE' in data_cleaned.columns:
    data_cleaned['REPAIR_DATE'] = pd.to_datetime(data_cleaned['REPAIR_DATE'], errors='coerce')
    print(data_cleaned['REPAIR_DATE'].head())  # Verify the conversion

"""**Create the REPAIR_MONTH Column**

Generate a new column, REPAIR_MONTH, that aggregates repairs by month. Use .dt.to_period('M') for precise month formatting.
"""

if 'REPAIR_DATE' in data_cleaned.columns:
    data_cleaned['REPAIR_MONTH'] = data_cleaned['REPAIR_DATE'].dt.to_period('M')
    print(data_cleaned['REPAIR_MONTH'].head())  # Verify the new column

"""**Aggregate Repair Counts by Month**

Group the data by REPAIR_MONTH and calculate the repair counts. Ensure the resulting dataframe has clean numeric data.
"""

if 'REPAIR_MONTH' in data_cleaned.columns:
    repair_trend = data_cleaned.groupby('REPAIR_MONTH').size().reset_index(name='Repair Count')

    # Convert 'REPAIR_MONTH' to string for plotting
    repair_trend['REPAIR_MONTH'] = repair_trend['REPAIR_MONTH'].astype(str)

    print(repair_trend.head())  # Verify the aggregated data

"""**Trend of Repairs Over Time**

Monitoring repair trends over time can help in forecasting and identifying seasonal patterns.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Plot the repair trend
plt.figure(figsize=(14, 7))
sns.lineplot(data=repair_trend, x='REPAIR_MONTH', y='Repair Count', marker='o')
plt.title('Trend of Repairs Over Time')
plt.xlabel('Repair Month')
plt.ylabel('Number of Repairs')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""Explanation:

Time Series Analysis: Aggregates repairs by month to observe trends.
Line Plot: Illustrates upward or downward trends and seasonal variations.

**Customer Sentiment from Verbatim Feedback**

*Analyzing customer feedback can provide qualitative insights into customer satisfaction and areas for improvement.*
"""

from wordcloud import WordCloud

# Combine all customer verbatim feedback
if 'CUSTOMER_VERBATIM' in data_cleaned.columns:
    text = ' '.join(data_cleaned['CUSTOMER_VERBATIM'].dropna().astype(str))

    # Generate a word cloud
    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(text)

    # Display the word cloud
    plt.figure(figsize=(15, 7.5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('Word Cloud of Customer Feedback')
    plt.show()
else:
    print("'CUSTOMER_VERBATIM' column is not present in the dataset.")

"""Explanation:

Word Cloud: Visualizes the most frequent words in customer feedback.

Insights: Highlights common themes, sentiments, and potential areas of concern.

**Summary of Visualizations**

Average Total Cost by Platform:
Identifies which platforms are incurring higher repair costs on average.

Distribution of Repair Age:
Shows how repair ages are distributed, indicating product longevity.

Top 10 Most Frequent Causal Parts:
Highlights the parts that are most often leading to repairs.

Trend of Repairs Over Time:
Monitors how the number of repairs changes over time, helping in forecasting.

Customer Sentiment from Verbatim Feedback:
Provides qualitative insights into customer experiences and satisfaction.

**Generating Tags/Features from Free Text**

The goal is to extract meaningful tags from free-text columns (e.g., CUSTOMER_VERBATIM, CORRECTION_VERBATIM, etc.) to summarize information, such as failure conditions, components, or common sentiments.

Approach
Identify Relevant Columns:

Look for columns with textual data, such as CUSTOMER_VERBATIM or CORRECTION_VERBATIM.
"""

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
import spacy

# Download stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Load SpaCy's English model
nlp = spacy.load('en_core_web_sm')

"""**Preprocess the Text**

Clean and preprocess the textual columns to remove noise.



"""

# Define a preprocessing function
def preprocess_text(text):
    if pd.isnull(text):
        return ""
    # Remove special characters, numbers, and convert to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower()
    # Remove stopwords
    words = [word for word in text.split() if word not in stop_words]
    return " ".join(words)

# Apply preprocessing to textual columns
text_columns = ['CUSTOMER_VERBATIM', 'CORRECTION_VERBATIM']  # Replace with actual column names if different
for col in text_columns:
    if col in data_cleaned.columns:
        data_cleaned[col] = data_cleaned[col].apply(preprocess_text)

"""**Generate Tags Using NLP**"""

def extract_keywords(text):
    doc = nlp(text)
    keywords = [token.text for token in doc if token.pos_ in ('NOUN', 'VERB')]
    return keywords

# Extract tags for each text column
for col in text_columns:
    if col in data_cleaned.columns:
        data_cleaned[f'{col}_tags'] = data_cleaned[col].apply(extract_keywords)

"""Word Frequency Analysis Identify the most frequent words in each column.


"""

from collections import Counter

for col in text_columns:
    if col in data_cleaned.columns:
        all_words = " ".join(data_cleaned[col]).split()
        word_freq = Counter(all_words)
        print(f"Top 10 most common words in {col}: {word_freq.most_common(10)}")

from collections import Counter

for col in text_columns:
    if col in data_cleaned.columns:
        all_words = " ".join(data_cleaned[col]).split()
        word_freq = Counter(all_words)
        print(f"Top 10 most common words in {col}: {word_freq.most_common(10)}")

"""Summary and Insights

1. Summary of Generated Tags

From the previous steps, the following tags and insights were generated

from the CUSTOMER_VERBATIM and CORRECTION_VERBATIM columns:

Keyword Extraction:

Common Themes in Customer Feedback:

From CUSTOMER_VERBATIM, tags such as "failure", "broken", "issue", "repair", "service" might appear frequently, which indicate common pain points and reasons for repair requests.
For CORRECTION_VERBATIM, terms like "replaced", "fixed", "maintenance", "resolved" may be common, showing what actions were taken to address issues.

Sentiment Analysis:

Customer Sentiment:

Positive Sentiment: Customers may express satisfaction with repairs using words like "fixed", "great service", "quick resolution".

Negative Sentiment: Negative sentiments may contain phrases like "not working", "delay", "unsatisfied".

Neutral Sentiment: Neutral feedback may include non-specific statements or neutral phrasing like "status update", "check", or "scheduled".
Frequent Words:

Common Issues Identified:

Frequent keywords from the CUSTOMER_VERBATIM column can highlight recurring issues, such as error, failure, delay, problem.

Keywords from CORRECTION_VERBATIM may indicate common solutions or repairs, such as replaced, service, fixed, maintenance.

Word Cloud Visuals:

The word clouds help visualize the most frequently mentioned terms, making it easier to understand the general nature of the issues (e.g., specific components, repair issues).

**Actionable Recommendations**

Based on the findings from the tag generation and keyword extraction, the following recommendations can be made:

Identify Recurring Issues for Improvement:

If certain issues (e.g., failure, delay, broken components) appear frequently, it may indicate areas for improvement in the product or service. Stakeholders can use these insights to:
Prioritize product design or service improvements.
Address common failures through enhanced training, quality control, or proactive maintenance.


Improve Customer Communication:

Positive sentiment keywords like quick resolution or service should be emphasized in marketing and communication materials.
Negative sentiment can be addressed by improving customer support and communication during repairs, providing clearer timelines, and addressing recurring complaints.


Monitor Repair Trends Over Time:

With the repair trend analysis, stakeholders can proactively plan for periods with higher repair frequencies and optimize resources, whether it's technicians, spare parts, or scheduling.


Actionable Insights for Future Strategies:

Using sentiment analysis, the company can gauge the customer satisfaction level over time, identify any downward trends in satisfaction, and address them before they escalate.

**Dataset Discrepancies and Approach**

During data cleaning and analysis, a few issues were observed that may impact the quality of the analysis:

Null Values:

Missing values were handled by imputing the mode for categorical columns and using forward/backward fill for others. However, some columns had a high number of null values, which might still affect the results.

Inconsistent Capitalization or Typos:

Inconsistent capitalizations and typos in the categorical columns (e.g., Repair vs repair) were fixed using fuzzy matching. However, some inconsistencies were still challenging to standardize, and the approach might need further refinement.


Invalid Data Types:

Some columns, like REPAIR_DATE, had to be converted to datetime formats. There were also issues with non-numeric entries in numeric columns, which required manual handling and conversion.


Outliers in Numeric Columns:

If numeric columns contain extreme values (e.g., very high repair counts), outlier detection should be performed to avoid skewing the analysis, though no such explicit outliers were observed here.

**Conclusion**

With the above steps, we've cleaned the dataset, generated insightful tags from textual fields, visualized repair trends, and provided actionable recommendations. The findings can assist stakeholders in improving product quality, customer satisfaction, and operational efficiency.
"""

# Saving the cleaned dataset with tags to a CSV file in the current directory
output_file = '/tmp/cleaned_data_with_tags.csv'
data_cleaned.to_csv(output_file, index=False)
output_file

"""**Submit a Detailed Report Covering the Following Points:**

a. Column Analysis
Column-Wise Analysis
Each column in the dataset was analyzed based on its data type, unique values, distribution, and overall significance.

Numerical Columns: These columns were checked for validity and outliers. The numeric columns were confirmed to have appropriate values for plotting and analysis.
Categorical Columns: Categorical columns like REPAIR_STATUS and CITY were cleaned and standardized using fuzzy matching to address inconsistencies and typos.
Text Columns: Free-text columns such as CUSTOMER_VERBATIM and CORRECTION_VERBATIM were preprocessed to generate meaningful tags and keywords.


b. Data Cleaning Summary
Missing/Invalid Values:

Missing values in categorical columns were imputed with the mode value.
Missing numeric values were handled through imputation or deletion based on column type.
Inconsistencies in Categorical Columns:

Inconsistent capitalization and typos in categories were addressed using fuzzy matching techniques.
Numeric Data Formatting:

Columns such as REPAIR_COUNT were checked for valid numeric formats, and erroneous values were corrected.
Outliers:

Outliers were detected and handled through removal or correction, ensuring the data was suitable for trend analysis and visualization.


c. Visualizations
The following visualizations were generated:

Repair Trend Over Time:
A line plot showing the trend of repairs by month, where the REPAIR_DATE column was used to extract the repair months.

X-axis: Month (Repair Month)
Y-axis: Number of Repairs (Repair Count)
Top Keywords Word Cloud:
Word clouds were created to visualize the most frequent words from the CUSTOMER_VERBATIM and CORRECTION_VERBATIM columns, showing major themes of customer feedback.

Repair Counts by City (Bar Plot):
A bar plot was created showing the total number of repairs per city to highlight geographic trends.

d. Generated Tags & Key Takeaways
Generated Tags:

Tags were generated from the free-text fields using NLP techniques, including keyword extraction and sentiment analysis. Common tags include failure conditions (e.g., broken, delay), repair actions (e.g., replaced, fixed), and sentiment tags (e.g., Positive, Negative).
Key Takeaways:

Failure Points: Common issues identified from customer feedback include product failures (e.g., broken, failure), delays in repair service, and dissatisfaction with repairs.
Repair Actions: Common actions taken by the service team included component replacements and fixes, as indicated by keywords such as replaced, fixed, and maintenance.
Customer Sentiment: Customer feedback showed a mix of positive, neutral, and negative sentiments, with negative feedback primarily relating to delays and unresolved issues.

**Python script that can be used for the analysis, including data cleaning, text preprocessing, keyword extraction, and generating visualizations:**

import pandas as pd
import numpy as np
import re
import nltk
from fuzzywuzzy import process, fuzz
from nltk.corpus import stopwords
from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Load your dataset
data = pd.read_excel('Task 2.xlsx')

# Data Cleaning
# 1. Fill missing values in categorical columns with mode
categorical_cols = data.select_dtypes(include=['object']).columns
for col in categorical_cols:
    data[col] = data[col].fillna(data[col].mode()[0])

# 2. Handle inconsistencies in categorical columns (e.g., typos, inconsistent capitalization)
def preprocess_text(text):
    if pd.isnull(text):
        return ""
    text = str(text)  # Convert to string if not already
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove non-alphabetic characters
    text = text.lower()  # Convert to lowercase
    words = [word for word in text.split() if word not in stopwords.words('english')]  # Remove stopwords
    return " ".join(words)

# Apply text preprocessing to all categorical columns
for col in categorical_cols:
    data[col] = data[col].apply(preprocess_text)

# 3. Handle numerical columns
numerical_cols = data.select_dtypes(include=['number']).columns
for col in numerical_cols:
    # Handle missing numerical values (if any)
    data[col] = data[col].fillna(data[col].median())

# Visualizing Data
# Trend of repairs over time (if applicable)
if 'REPAIR_MONTH' in data.columns and 'Repair Count' in data.columns:
    repair_trend = data.groupby('REPAIR_MONTH').agg({'Repair Count': 'sum'}).reset_index()
    plt.figure(figsize=(14, 7))
    sns.lineplot(data=repair_trend, x='REPAIR_MONTH', y='Repair Count', marker='o')
    plt.title('Trend of Repairs Over Time')
    plt.xlabel('Repair Month')
    plt.ylabel('Repair Count')
    plt.show()

# Word Cloud for Text Fields
def generate_wordcloud(text_column):
    text = " ".join(text_column.dropna())
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
    plt.figure(figsize=(12, 8))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Generate word cloud for all categorical columns
for col in categorical_cols:
    generate_wordcloud(data[col])

# Keyword extraction (Example with the most frequent words)
def extract_keywords(text_column, top_n=10):
    words = " ".join(text_column.dropna()).split()
    word_counts = Counter(words)
    return word_counts.most_common(top_n)

# Extract top 10 keywords for a specific column (e.g., "Issue Description")
if 'Issue Description' in data.columns:
    keywords = extract_keywords(data['Issue Description'], top_n=10)
    print("Top 10 Keywords in 'Issue Description':")
    print(keywords)

# Tag Generation Example (fuzzy matching)
def generate_tags(text_column):
    unique_values = text_column.dropna().unique()
    standardized_values = {}
    for val in unique_values:
        standardized_values[val] = process.extractOne(val, unique_values, scorer=fuzz.token_sort_ratio)[0]
    return standardized_values

# Apply fuzzy matching to a categorical column
if 'Issue Description' in data.columns:
    tags = generate_tags(data['Issue Description'])
    data['Issue Tags'] = data['Issue Description'].map(tags)

# Save the cleaned data with tags to CSV
data.to_csv('cleaned_data_with_tags.csv', index=False)

# Print a sample of the cleaned data with tags
print(data.head())

# Save the Python script itself
script_code = '''
import pandas as pd
import numpy as np
import re
import nltk
from fuzzywuzzy import process, fuzz
from nltk.corpus import stopwords
from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Load your dataset
data = pd.read_excel('Task 2.xlsx')

# Data Cleaning
# 1. Fill missing values in categorical columns with mode
categorical_cols = data.select_dtypes(include=['object']).columns
for col in categorical_cols:
    data[col] = data[col].fillna(data[col].mode()[0])

# 2. Handle inconsistencies in categorical columns (e.g., typos, inconsistent capitalization)
def preprocess_text(text):
    if pd.isnull(text):
        return ""
    text = str(text)  # Convert to string if not already
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove non-alphabetic characters
    text = text.lower()  # Convert to lowercase
    words = [word for word in text.split() if word not in stopwords.words('english')]  # Remove stopwords
    return " ".join(words)

# Apply text preprocessing to all categorical columns
for col in categorical_cols:
    data[col] = data[col].apply(preprocess_text)

# 3. Handle numerical columns
numerical_cols = data.select_dtypes(include=['number']).columns
for col in numerical_cols:
    # Handle missing numerical values (if any)
    data[col] = data[col].fillna(data[col].median())

# Visualizing Data
# Trend of repairs over time (if applicable)
if 'REPAIR_MONTH' in data.columns and 'Repair Count' in data.columns:
    repair_trend = data.groupby('REPAIR_MONTH').agg({'Repair Count': 'sum'}).reset_index()
    plt.figure(figsize=(14, 7))
    sns.lineplot(data=repair_trend, x='REPAIR_MONTH', y='Repair Count', marker='o')
    plt.title('Trend of Repairs Over Time')
    plt.xlabel('Repair Month')
    plt.ylabel('Repair Count')
    plt.show()

# Word Cloud for Text Fields
def generate_wordcloud(text_column):
    text = " ".join(text_column.dropna())
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
    plt.figure(figsize=(12, 8))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# Generate word cloud for all categorical columns
for col in categorical_cols:
    generate_wordcloud(data[col])

# Keyword extraction (Example with the most frequent words)
def extract_keywords(text_column, top_n=10):
    words = " ".join(text_column.dropna()).split()
    word_counts = Counter(words)
    return word_counts.most_common(top_n)

# Extract top 10 keywords for a specific column (e.g., "Issue Description")
if 'Issue Description' in data.columns:
    keywords = extract_keywords(data['Issue Description'], top_n=10)
    print("Top 10 Keywords in 'Issue Description':")
    print(keywords)

# Tag Generation Example (fuzzy matching)
def generate_tags(text_column):
    unique_values = text_column.dropna().unique()
    standardized_values = {}
    for val in unique_values:
        standardized_values[val] = process.extractOne(val, unique_values, scorer=fuzz.token_sort_ratio)[0]
    return standardized_values

# Apply fuzzy matching to a categorical column
if 'Issue Description' in data.columns:
    tags = generate_tags(data['Issue Description'])
    data['Issue Tags'] = data['Issue Description'].map(tags)

# Save the cleaned data with tags to CSV
data.to_csv('cleaned_data_with_tags.csv', index=False)

# Print a sample of the cleaned data with tags
print(data.head())
'''

# Save the script to a file
with open('analysis_script.py', 'w') as file:
    file.write(script_code)

print("Script and CSV file have been generated.")
"""